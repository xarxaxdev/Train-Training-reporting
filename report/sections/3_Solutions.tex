
\section{Solutions}
Every encoding below uses an estimate of the horizon as window for its computations, which is passed from the flatland environments.

\subsection{Graph}
\noindent \textbf{Motivation} The basic concept here is to convert Flatland instances into an abstract graph representation. The main difficulty lies therein, that in Flatland cells are not the only thing constraining movement, but additionally a direction needs to either be tracked as additional information, or be merged into the graph. For this encoding we chose the second approach. To ensure a graph-like structure vertices are not defined by just a cell, but also from where it was reached. This leads to a graph with directed edges and paths modelling the environment. This approach serves as a baseline for comparison, and a foundation to build upon with the other encodings.\\

\begin{figure}
\begin{minipage}[t]{0.45\textwidth}
    \centering
	\begin{tikzpicture}
		\draw[blue] (0,0.5) rectangle (1,1.5);
		\draw (0.5,0.5) -- (0.5,1.5);
		\draw (0.5,0.5) to[out=90,in=180] (1,1);
	
		\draw[blue] (3,1.5) rectangle (4,2.5);
		\draw (3.5,1.5) -- (3.5,2.5);
		\draw (3.5,1.5) to[out=90,in=180] (4,2);
		\draw (3,0.5) rectangle (4,1.5);
		\draw[-{Stealth},color=red,thick] (3.5,1.5) -- (3.5,1.7);
	
		\draw[blue] (4.5,0.5) rectangle (5.5,1.5);
		\draw (5,0.5) -- (5,1.5);
		\draw (5,0.5) to[out=90,in=180] (5.5,1);
		\draw (4.5,1.5) rectangle (5.5,2.5);
		\draw[-{Stealth},color=red,thick] (5,1.5) -- (5,1.3);
	
		\draw[blue] (6,0.5) rectangle (7,1.5);
		\draw (6.5,0.5) -- (6.5,1.5);
		\draw (6.5,0.5) to[out=90,in=180] (7,1);
		\draw (7,0.5) rectangle (8,1.5);
		\draw[-{Stealth},color=red,thick] (7,1) -- (6.8,1);
	
		\node at (0.2, 1.4) {At};
		\node at (3.2, 2.4) {At};
		\node at (4.7, 1.4) {At};
		\node at (6.2, 1.4) {At};
		\node at (3.35, 1.4) {Prev};
		\node at (4.85, 2.4) {Prev};
		\node at (7.35, 1.4) {Prev};
	
		\draw[->,thick] (1.5,1) -- (2.5,1);
	\end{tikzpicture}
    \caption{Cell to Graph}
    \label{fig:graph}
\end{minipage}
\end{figure}

\noindent \textbf{Implementation} To that end, the information from flatland is converted similar to Figure \ref{fig:graph}. The vertex predicates are \texttt{vertex(At,Prev)}, where \texttt{At} is the position of the train, and \texttt{Prev} is a possible previous position. As the red arrows show, one could also replace \texttt{Prev} with the facing direction. Both contain the same information. With this, one cell is represented by up to 4 vertices. This is handled by introducing a predicate shared resources, to forbid conflicts.

This approach is a rudimentary one, representing agents and their position, for every timestep. As many papers deal with Multi Agent Pathfinding Problems on ASP, I will not go into more detail here, but instead skip to the adaption of malfunctions. (More Info on Solving MAPF with ASP: \cite{MAPF})\\

\noindent The secondary encoding handles malfunctions in three steps.

\textit{Reconstruction} relies on the previous encoding passing the path and uses it together with the malfunction time, to reconstruct the current state. To that end, the plan is copied for all timesteps up to malfunction time. Except the malfunctioning trains and trains which collide with them. For those the last action fails, therefore one action less is copied. 

\textit{Persisting} computes how long a train malfunctions and derives persistent information which will never be violated even if another malfunction happens in the next timestep. This makes the encoding remember malfunctions and the following information (like train 0 malfunctions there for 5 timesteps), without enforcing them again.

\textit{Recompute} repeats the pathfinding computation with a shifted window. The window starts at the malfunction timesteps and exceeds the old window by the duration of the worst malfunction at the current timestep. This assumes the worst case, as a malfunction for 5 timesteps, at worst requires 5 additional timesteps to solve the instance.\\

To save on computation, the secondary encodings does not compute the underlying graph again, but instead gets it passed via save/load, thus receiving the derived atoms as input.

\noindent \textbf{Implications} This might lead to an explosion of window size. If for example multiple trains, successively encounter a malfunction of 1 timestep, while not interfering with one another, they would still require one additional timestep to ensure solvability. Instead, this encoding would add as many timesteps as trains, complicating computation.

As this encoding models the paths by timesteps it scales badly with bigger instances, leading to an explosion in grounding.

Lastly, it might not be optimal but given a good guess for the horizon, it can compute a valid path without any optimality quickly.

\subsection{Naive Graph}
\noindent \textbf{Motivation} To ascertain whether, passing the graph-predicates instead of just passing the environment-predicates and recomputing it, improves runtime this encoding was implemented.\\

\noindent \textbf{Implementation} As the secondary encoding receives its sole input from the previous runs it is necessary to load and save the environment predicates on every run. These are then used to compute the graph as well as a solution. Other than that it mirrors the graph approach.\\

\noindent \textbf{Implications} This approach will probably end up being slower than the baseline. As reading the graph from predicates should be slower than computing it. Other than that it should behave quite similarly to the graph approach.

\subsection{Incremental}
\textbf{Motivation} Incremental encodings as described in \cite{incr} allow for incrementally adding timesteps to the grounding until a problem is solvable. The advantage of that is, if the amount of time required to solve a problem is unknown, this allows to solve it with the minimal amount of timesteps, not requiring an estimated horizon. The main downside is, that every iteration requires it to prove unsatisfiability before adding the next timestep. Thus, it is mostly applicable, when lacking a good estimate of the horizon.

Nonetheless, this approach should be well suited, for this problem. As instead of being run once, it is run multiple times for every malfunction, where every unnecessary timestep extends the time taken over and over. \\

\noindent \textbf{Implementation} The resulting implementation uses the horizon estimate as the other encodings do, but whenever this leads to unsatisfiability another timestep is added. Thus, still using the given estimate and not having to prove unsatisfieability too often. The incremental implementation is mainly used for malfunction handling. In the case of a malfunction it first tries to solve it, without adding timesteps and only adds them if required. 

This required work on the repository side, where \textit{Clingo} is called. Since the repository calls the Python \textit{Clingo} API, it is not possible to call python from \textit{Clingo} again. So Felix needed to modify the given call in api.py. The result can be seen in our GitHub Repository and follows closely the implementation in \cite{incr}.

Next the encoding was separated into three parts. A \texttt{base} program, which would run always. A \texttt{step} program, which would only be called if necessary and a \texttt{check}, which contains integrity constraints, which need to be checked for the entire program and not just parts of it. The graph generation and pathfinding up to the first horizon was moved to the \texttt{base} program. Then the same pathfinding is implemented for the \texttt{step} where it only grounds one step at a time. Lastly the goal condition of every agent reaching its goal, was implemented in the \texttt{check} part of the program, which is grounded before every solving and removed afterwards.
\color{red} Then a stepwise pathfinding (mimicking the other) is implemented for the step and grounds one step at a time. -> WHAT \color{black} \color{green} Is it better now? \color{black}

Representation wise, It is an extension of the graph approach and uses the same building and passing of the graph.\\

\noindent \textbf{Implications} As it solves over and over, this encoding should perform worse in some timesteps, where the adding of additional timesteps, is required.

Other than that it should, outperform the other encodings. This should hold especially for environments dense in malfunctions, where the worst case assumption should add way to many additional timesteps.

\color{red} Should it not perform worse when e.g.  very disjointed windows (start,end) happen? As a problem becomes less NP and more individual problems, it should perform worse \color{black} \color{green} Confusion \color{black}
